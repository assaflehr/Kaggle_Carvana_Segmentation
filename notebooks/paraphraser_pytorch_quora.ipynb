{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paraphraser_pytorch_quora.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/Kaggle_Carvana_Segmentation/blob/master/notebooks/paraphraser_pytorch_quora.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XYOemVe_MYXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "1521d4dc-17d2-43ad-a1bf-413a668b8f8b"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch -U  # 0.4 at-least, on windows, read installation instrcution of pytorch.org\n",
        "!pip install --quiet torchtext  # for simpler datasets\n",
        "!pip install --quiet git+https://github.com/IBM/pytorch-seq2seq  #for seq2seq\n",
        "!pip install --quiet dill  #req of seq2seq\n",
        "!pip install --quiet tqdm  #req of seq2seq\n",
        "!pip install --quiet revtok #for torchtext word tokenizer\n",
        "\n",
        "#quora dataset\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"myusername\"\n",
        "os.environ['KAGGLE_KEY'] = \"mykey\"\n",
        "!kaggle competitions download -c quora-question-pairs\n",
        "!unzip train.csv.zip\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Collecting git+https://github.com/IBM/pytorch-seq2seq\n",
            "  Cloning https://github.com/IBM/pytorch-seq2seq to /tmp/pip-req-build-dyh9g0kx\n",
            "Requirement already satisfied (use --upgrade to upgrade): seq2seq==0.1.6 from git+https://github.com/IBM/pytorch-seq2seq in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.4.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (4.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2.6)\n",
            "Building wheels for collected packages: seq2seq\n",
            "  Running setup.py bdist_wheel for seq2seq ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-b3d51p8m/wheels/98/b5/06/771c406b3ecc8ed34f07da72d7baf65b87e561bd9f808e91bd\n",
            "Successfully built seq2seq\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/78/832b9a9ec6b3baf8ec566e1f0a695f2fd08d2c94a6797257a106304bfc3c/kaggle-1.4.7.1.tar.gz (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.8.24)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.26.0)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 3.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/44/2c/df/22a6eeb780c36c28190faef6252b739fdc47145fd87a6642d4\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.4.7.1 python-slugify-1.2.6\n",
            "Downloading test.csv.zip to /content\n",
            " 88% 153M/173M [00:05<00:00, 25.6MB/s]\n",
            "100% 173M/173M [00:05<00:00, 31.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "100% 4.95M/4.95M [00:00<00:00, 24.3MB/s]\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            " 79% 17.0M/21.5M [00:00<00:00, 17.2MB/s]\n",
            "100% 21.5M/21.5M [00:00<00:00, 25.8MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0bXj8NKRuPtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "812d8299-fa28-4cb2-be6e-1a56f90e797a"
      },
      "cell_type": "code",
      "source": [
        "!rm -r paraphraser_pytorch  #remove previous github copy if needed\n",
        "!git clone https://github.com/assaflehr/paraphraser_pytorch.git\n",
        "import sys\n",
        "sys.path.append('paraphraser_pytorch/paraph')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paraphraser_pytorch'...\n",
            "remote: Counting objects: 96, done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 96 (delta 47), reused 76 (delta 27), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aFKCNNnp3llN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#option 1 : Run from command-line"
      ]
    },
    {
      "metadata": {
        "id": "vpc4LoyOuRTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "e6d03576-dfd3-4877-fdc1-242d91127feb"
      },
      "cell_type": "code",
      "source": [
        "! python paraphraser_pytorch/paraph/train.py --help\n",
        "\n",
        "! python paraphraser_pytorch/paraph/train.py --dataset quora"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--dataset DATASET] [--epocs EPOCS]\n",
            "                [--epoch_size EPOCH_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--optimizer OPTIMIZER] [--lr LR] [--adv_disc_lr ADV_DISC_LR]\n",
            "                [--beta1 BETA1] [--semantics_dim SEMANTICS_DIM]\n",
            "                [--style_dim STYLE_DIM] [--sd_weight SD_WEIGHT]\n",
            "                [--sem_sim_weight SEM_SIM_WEIGHT]\n",
            "                [--max_sent_len MAX_SENT_LEN]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataset DATASET     dataset to use. valid values are quora or bible\n",
            "  --epocs EPOCS         number of epochs to train for\n",
            "  --epoch_size EPOCH_SIZE\n",
            "                        epoch size\n",
            "  --batch_size BATCH_SIZE\n",
            "                        batch size\n",
            "  --optimizer OPTIMIZER\n",
            "                        optimizer to train with. only Adam supported.\n",
            "  --lr LR               learning rate. 0.001 is a food value\n",
            "  --adv_disc_lr ADV_DISC_LR\n",
            "                        learning rate\n",
            "  --beta1 BETA1         momentum term for adam\n",
            "  --semantics_dim SEMANTICS_DIM\n",
            "                        size of the semantics vector.default 256\n",
            "  --style_dim STYLE_DIM\n",
            "                        size of the style vector.default 1\n",
            "  --sd_weight SD_WEIGHT\n",
            "                        weight on adversarial loss 0.0001 originally. 0.5 is\n",
            "                        good value!\n",
            "  --sem_sim_weight SEM_SIM_WEIGHT\n",
            "                        weight on semantic similiarity loss\n",
            "  --max_sent_len MAX_SENT_LEN\n",
            "                        max size of sentence. sentences typically will be\n",
            "                        shorter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Inutsgik35m8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Option 2 run from code directly"
      ]
    },
    {
      "metadata": {
        "id": "j6YTtj0vvKq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2855
        },
        "outputId": "d064a87b-4722-49bb-908a-494ac3f33725"
      },
      "cell_type": "code",
      "source": [
        "from train import train_main\n",
        "from options import get_options\n",
        "\n",
        "sys.argv= [\"first_is_filename\",\"--dataset\",\"quora\",\"--sem_sim_weight\",\"0.01\",\"--sd_weight\",\"10\",\"--epocs\",\"100\",\"--lr\",\"0.001\"] #change if needed\n",
        "bucket_iter_train, bucket_iter_val, models= train_main()\n",
        "#now you can query the iterators, look inside the model summary, etc \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running train with options: Namespace(adv_disc_lr=0.01, batch_size=32, beta1=0.5, dataset='quora', epoch_size=500, epocs=100, lr=0.001, max_sent_len=30, optimizer='adam', sd_weight=10.0, sem_sim_weight=0.01, semantics_dim=256, style_dim=100)\n",
            "dups 149650, not_dups 413109\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? '], sent_1=[' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? '], sent_x=[' i ', ' want ', ' to ', ' start ', ' writing ', '. ', ' how ', ' do ', ' i ', ' begin ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? '], sent_1=[' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? '], sent_x=[' how ', ' can ', ' i ', ' stop ', ' having ', ' morning ', ' diarrhea ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? '], sent_1=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], sent_x=[' what ', ' are ', ' the ', ' funniest ', ' jokes ', ' / ', ' stories ', ' you ', ' ever ', ' heard ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], sent_1=[' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? '], sent_x=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], is_x_0=0.0, sent_0_target=['<sos>', ' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? '], sent_1=[' how ', ' can ', ' i ', ' see ', ' all ', ' my ', ' youtube ', ' comments ', '? '], sent_x=[' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? '], is_x_0=0.0, sent_0_target=['<sos>', ' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? ', '<eos>'])\n",
            "printing dataset directly, before tokenizing:\n",
            "sent_0 [' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? ']\n",
            "is_x_0 1.0\n",
            "\n",
            "building vocab:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "09:07:45 INFO:Loading vectors from .vector_cache/wiki.simple.vec.pt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab TEXT: len 29464 common [('? ', 155779), ('<sos>', 149650), ('<eos>', 149650), (' the ', 64718), (' what ', 62187), (' is ', 48178), (' how ', 40218), (' i ', 33919), (' a ', 32373), (' to ', 32224)]\n",
            "vocab TEXT_TARGET: 29464 [('? ', 155779), ('<sos>', 149650), ('<eos>', 149650), (' the ', 64718), (' what ', 62187), (' is ', 48178), (' how ', 40218), (' i ', 33919), (' a ', 32373), (' to ', 32224)]\n",
            "vocab  <sos> 2 2\n",
            "vocab  <eos> 3 3\n",
            "vocab  out-of-vocab 3 0\n",
            "training with lr 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "09:10:28 INFO:[00] train rec loss: 4.0046 | sim loss: 0.0064 | anti_disc_loss: 0.0002 || scene disc 0.6929 51.131% \n",
            "09:11:40 INFO:[00] eval rec loss: 3.5195 | sim loss: 0.0054 | anti_disc_loss: 0.0003 || scene disc 0.6928 52.663% \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0:  what  is  the  best  way  to  teach  kids  alphabets ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> what  is  the  best  way  to  teach  kids  alphabets ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  how  can  i  teach  my  kids  the  alphabet ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  where  can  i  find  a  video  of  steve  jobs  announcing  the  iphone ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> what  is  the  best  way  to  learn  a  job ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> what  is  the  best  way  to  learn  a  job ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> how  can  i  get  a  job ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> how  can  your  best  way  to  learn  a ? ? <eos><eos><eos><eos>? ? ? ? ? ? ? ? ? ? ? ? \n",
            "\n",
            "              sent0:  how  do  i  delete  a  friend  list  on  facebook ?  <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> how  do  i  delete  a  friend  list  on  facebook ?  <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  how  do  i  delete  friends  list  in  facebook ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  how  do  i  delete  a  friend  list  on  facebook ?  <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> how  do  i  get  a  transit  visa ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> how  do  i  get  a  transit  visa ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> how  do  i  get  a  transit  visa ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> how  do  i  get  my  good ? ?  quora ? <eos><eos><eos><eos><eos>? ? ? ? ? ? ? ? ? ? ? \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "09:15:02 INFO:[01] train rec loss: 3.3675 | sim loss: 0.0057 | anti_disc_loss: 0.0004 || scene disc 0.6923 54.237% \n",
            "09:17:49 INFO:[02] train rec loss: 3.1030 | sim loss: 0.0060 | anti_disc_loss: 0.0008 || scene disc 0.6907 58.156% \n",
            "09:20:30 INFO:[03] train rec loss: 2.8832 | sim loss: 0.0062 | anti_disc_loss: 0.0014 || scene disc 0.6885 61.831% \n",
            "09:23:08 INFO:[04] train rec loss: 2.6714 | sim loss: 0.0065 | anti_disc_loss: 0.0026 || scene disc 0.6858 64.044% \n",
            "09:25:50 INFO:[05] train rec loss: 2.6240 | sim loss: 0.0071 | anti_disc_loss: 0.0042 || scene disc 0.6839 64.575% \n",
            "09:28:30 INFO:[06] train rec loss: 2.4600 | sim loss: 0.0071 | anti_disc_loss: 0.0075 || scene disc 0.6805 65.056% \n",
            "09:31:10 INFO:[07] train rec loss: 2.3598 | sim loss: 0.0082 | anti_disc_loss: 0.0113 || scene disc 0.6771 65.475% \n",
            "09:33:48 INFO:[08] train rec loss: 2.2379 | sim loss: 0.0085 | anti_disc_loss: 0.0148 || scene disc 0.6751 65.888% \n",
            "09:36:27 INFO:[09] train rec loss: 2.1734 | sim loss: 0.0093 | anti_disc_loss: 0.0218 || scene disc 0.6716 65.750% \n",
            "09:39:12 INFO:[10] train rec loss: 2.1130 | sim loss: 0.0096 | anti_disc_loss: 0.0260 || scene disc 0.6696 65.812% \n",
            "09:40:24 INFO:[10] eval rec loss: 2.0048 | sim loss: 0.0087 | anti_disc_loss: 0.0288 || scene disc 0.6679 65.662% \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0:  what  is  the  best  doctor  who  episode  for  nonviewers ? <pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> what  is  the  best  doctor  who  episode  for  nonviewers ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  what  is  the  best  doctor  who  episode  to  get  someone  addicted  to  the  series ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  best  of  x :  what  are  the  best  episodes  of  doctor  who ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> what  is  the  best  gift  for  christmas ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> what  is  the  best  song  for  your  favorite  band ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> what  is  the  best  way  to  learn  how  to  the  doctor ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> what  is  the  best  way  to  have  the  the ? <eos><eos><eos> it  the  the ? ? ? ? \n",
            "\n",
            "              sent0:  do  girls  experience  pain  during  first  time  sex ? <pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> do  girls  experience  pain  during  first  time  sex ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  how  can  pain  be  reduced  during  first  sex ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  do  girls  experience  pain  during  first  time  sex ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> do  girls  masturbate  at  night ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> do  girls  masturbate  at  night ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> how  can  one  be  reduced  from  depression ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> how  women  fall  the  with  pregnancy  time ? ? <eos><eos><eos><eos><eos>? ? ? ? ? ? \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "09:43:45 INFO:[11] train rec loss: 2.0790 | sim loss: 0.0102 | anti_disc_loss: 0.0338 || scene disc 0.6675 64.894% \n",
            "09:46:25 INFO:[12] train rec loss: 2.0088 | sim loss: 0.0106 | anti_disc_loss: 0.0432 || scene disc 0.6668 64.338% \n",
            "09:49:07 INFO:[13] train rec loss: 1.9851 | sim loss: 0.0108 | anti_disc_loss: 0.0477 || scene disc 0.6650 64.713% \n",
            "09:51:47 INFO:[14] train rec loss: 1.9078 | sim loss: 0.0114 | anti_disc_loss: 0.0559 || scene disc 0.6658 63.837% \n",
            "09:54:26 INFO:[15] train rec loss: 1.8586 | sim loss: 0.0118 | anti_disc_loss: 0.0609 || scene disc 0.6653 63.325% \n",
            "09:57:06 INFO:[16] train rec loss: 1.8198 | sim loss: 0.0122 | anti_disc_loss: 0.0738 || scene disc 0.6599 64.856% \n",
            "09:59:49 INFO:[17] train rec loss: 1.8293 | sim loss: 0.0127 | anti_disc_loss: 0.0816 || scene disc 0.6595 64.831% \n",
            "10:02:30 INFO:[18] train rec loss: 1.8205 | sim loss: 0.0125 | anti_disc_loss: 0.0905 || scene disc 0.6609 63.862% \n",
            "10:05:06 INFO:[19] train rec loss: 1.7213 | sim loss: 0.0130 | anti_disc_loss: 0.0953 || scene disc 0.6592 64.300% \n",
            "10:07:47 INFO:[20] train rec loss: 1.7325 | sim loss: 0.0130 | anti_disc_loss: 0.1012 || scene disc 0.6602 63.294% \n",
            "10:08:58 INFO:[20] eval rec loss: 1.5920 | sim loss: 0.0117 | anti_disc_loss: 0.0807 || scene disc 0.6501 67.694% \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0:  what  do  you  think  about  quora ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> what  do  you  think  about  quora ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  what  do  you  think  about  quora  in  general ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  how  can  i  fix  my  laptop  that  won ' t  turn  on  anymore ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> what  do  you  think  about  quora ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> do  you  think  mitt  romney  should  i  purchase ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> do  you  think  mitt  romney  is  good  for  losing ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> do  do  you  think  about  me ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "\n",
            "              sent0:  what  kinds  of  gifts  do  parents  give  to  their  young  children ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> what  kinds  of  gifts  do  parents  give  to  their  young  children ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  what  kinda  of  gifts  do  parents  give  to  their  young  children ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  what  are  the  most  important  skills  for  leadership ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> what  kind  of  do  you  have  to  give  me  to  meet  your  family ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> what  kind  of  do  you  think  of  \" do  you  have  to  learn ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> what  kind  of  do  you  think  about  homeschooling ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> what  kind  of  do  do  you  have  to  improve  interests ? ? <eos><eos><eos><eos><eos><eos>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10:12:17 INFO:[21] train rec loss: 1.7303 | sim loss: 0.0132 | anti_disc_loss: 0.1097 || scene disc 0.6593 63.888% \n",
            "10:14:54 INFO:[22] train rec loss: 1.6698 | sim loss: 0.0133 | anti_disc_loss: 0.1104 || scene disc 0.6596 63.731% \n",
            "10:17:32 INFO:[23] train rec loss: 1.6634 | sim loss: 0.0136 | anti_disc_loss: 0.1300 || scene disc 0.6646 61.906% \n",
            "10:20:11 INFO:[24] train rec loss: 1.6703 | sim loss: 0.0139 | anti_disc_loss: 0.1420 || scene disc 0.6658 61.612% \n",
            "10:22:49 INFO:[25] train rec loss: 1.6075 | sim loss: 0.0141 | anti_disc_loss: 0.1230 || scene disc 0.6596 63.644% \n",
            "10:25:27 INFO:[26] train rec loss: 1.6048 | sim loss: 0.0143 | anti_disc_loss: 0.1186 || scene disc 0.6624 62.712% \n",
            "10:28:06 INFO:[27] train rec loss: 1.5696 | sim loss: 0.0145 | anti_disc_loss: 0.1568 || scene disc 0.6652 61.825% \n",
            "10:30:45 INFO:[28] train rec loss: 1.5490 | sim loss: 0.0145 | anti_disc_loss: 0.1175 || scene disc 0.6571 63.925% \n",
            "10:33:26 INFO:[29] train rec loss: 1.5699 | sim loss: 0.0146 | anti_disc_loss: 0.1287 || scene disc 0.6625 63.069% \n",
            "10:36:02 INFO:[30] train rec loss: 1.5515 | sim loss: 0.0147 | anti_disc_loss: 0.1452 || scene disc 0.6615 62.856% \n",
            "10:37:11 INFO:[30] eval rec loss: 1.4431 | sim loss: 0.0129 | anti_disc_loss: 0.0991 || scene disc 0.6612 62.206% \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0:  should  one  clear  credit  card  debt  or  shiuld  give  down  payment  for  a  first  house ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> should  one  clear  credit  card  debt  or  shiuld  give  down  payment  for  a  first  house ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  should  one  clear  credit  card  debt  or  should  give  down  payment  for  a  first  house ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  which  startups  are  hiring  in  mumbai ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> should  i  buy  a  year - up  to  customer  service  or  will  i  purchase  for  a  business / 5 lakhs ? <eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> which  are  some  good  startup  companies  in  the  startup  of  india ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> which  are  some  good  startup  companies  in  the  startup  of  companies ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> which  i  start  or  or  or  or  a ?  a ? ?  the  startup  startup ? <eos><eos><eos><eos><eos>? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "\n",
            "              sent0:  what  manufacturing  company  could  i  contact  to  develop  my  digital  product  idea ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> what  manufacturing  company  could  i  contact  to  develop  my  digital  product  idea ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  what  design  and  product  manufacturing  companies  could  help  with  the  development  of  my  product  idea ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  what  are  the  best  first  class  train  to  travel  from  trivandrum  to  bangalore ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> what  would  happen  if  i  start  my  own  company ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> what  would  be  my  strategy  to  get  a  tomorrowland  of  flying  jatt ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> what  would  happen  if  i  start  ballet  and  drop  out  of  the  campus  freely ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> what  would  would  would  i  survive  with  rent  the  own  footprint  in  in <eos><eos><eos><eos><eos> in  in  in  in  in  in  in  in  in  in  in  in  in  in  in  in  in \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10:40:27 INFO:[31] train rec loss: 1.5524 | sim loss: 0.0151 | anti_disc_loss: 0.1738 || scene disc 0.6651 61.469% \n",
            "10:43:08 INFO:[32] train rec loss: 1.5209 | sim loss: 0.0150 | anti_disc_loss: 0.1328 || scene disc 0.6645 61.775% \n",
            "10:45:46 INFO:[33] train rec loss: 1.4930 | sim loss: 0.0152 | anti_disc_loss: 0.1768 || scene disc 0.6611 63.200% \n",
            "10:48:28 INFO:[34] train rec loss: 1.4893 | sim loss: 0.0156 | anti_disc_loss: 0.2453 || scene disc 0.6669 60.569% \n",
            "10:51:08 INFO:[35] train rec loss: 1.5092 | sim loss: 0.0154 | anti_disc_loss: 0.2491 || scene disc 0.6742 59.806% \n",
            "10:53:45 INFO:[36] train rec loss: 1.4694 | sim loss: 0.0158 | anti_disc_loss: 0.1944 || scene disc 0.6675 61.462% \n",
            "10:56:20 INFO:[37] train rec loss: 1.4492 | sim loss: 0.0152 | anti_disc_loss: 0.1818 || scene disc 0.6675 60.606% \n",
            "10:59:00 INFO:[38] train rec loss: 1.4580 | sim loss: 0.0154 | anti_disc_loss: 0.1556 || scene disc 0.6626 62.487% \n",
            "11:01:42 INFO:[39] train rec loss: 1.4314 | sim loss: 0.0157 | anti_disc_loss: 0.2643 || scene disc 0.6745 59.119% \n",
            "11:04:22 INFO:[40] train rec loss: 1.4569 | sim loss: 0.0155 | anti_disc_loss: 0.2380 || scene disc 0.6725 58.525% \n",
            "11:05:33 INFO:[40] eval rec loss: 1.3292 | sim loss: 0.0133 | anti_disc_loss: 0.0725 || scene disc 0.6689 62.306% \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0:  which  is  the  best  mouse  under  rs  1000 ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> which  is  the  best  mouse  under  rs  1000 ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  which  is  the  best  mouse  i  can  get  for  under  rs . 1000 ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  what  is  the  process  of  admissions  for  indians  who  want  to  get  into  mit ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> which  is  the  best  earphones  under  rs  1000  rs  500  and  rs  1000  notes ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> which  is  the  best  school  college  education ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> what  is  the  procedure  for  getting  a  school  education ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> what  is  the  best  college  for  college .  to  should <eos><eos> is . . . . .  the  the  the  the  the  the  the  the ? ? ? ? ? ? ? \n",
            "\n",
            "              sent0:  why  don ' t  people  simply  ' google '  instead  of  asking  questions  on  quora ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos><sos> why  don ' t  people  simply  ' google '  instead  of  asking  questions  on  quora ? <eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1:  why  do  so  many  people  ask  google - able  questions  on  quora ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX:  what  is  the  stupidest  thing  you ' ve  done  while  being  drunk ? <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] <sos> why  do  people  see  quora  when  i  don ' t  know  about  google ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty0:[TF=0] <sos> why  do  people  like  that  the  night ' s  watch  after  recorded ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=0] <sos> what ' s  your  most  embarrassing  thing  you  have  said ? <eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
            "recon_semX_sty1:[TF=1] <sos> what  do ' t  people  like  enjoy  their '  while  of  days  pregnant ?  drugs ? <eos><eos><eos><eos> you  you  you  you  you  you  you  you  you  you  you  you  you \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}